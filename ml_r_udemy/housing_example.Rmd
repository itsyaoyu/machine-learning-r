---
title: "Housing Example"
author: "Yao Yu"
date: "7/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(mice)
library(dummies)
library(tidymodels)
library(readxl)
library(janitor)
library(ggcorrplot)

# housing_data <- read_csv("Data Files/Linear Regression Dataset/House_Price.csv")
housing_data <- read_csv("Data Files/Logistic Reg Dataset/House-Price.csv")
```

```{r cleaning outliers}
# Capping and flooring outliers in n_hot_rooms and rainfall
n_hot_rooms_upper <- 3 * quantile(housing_data$n_hot_rooms, .99)
rainfall_lower <- 0.3 * quantile(housing_data$rainfall, .01)

housing_data <- housing_data %>% 
  mutate(n_hot_rooms = ifelse(n_hot_rooms > n_hot_rooms_upper, n_hot_rooms_upper, n_hot_rooms),
         rainfall = ifelse(rainfall < rainfall_lower, rainfall_lower, rainfall))

housing_data %>% 
  ggplot(aes(x = n_hot_rooms)) +
  geom_histogram()
```

```{r replace missing values with imputation}
# Using mice package for imputation instead of just average
housing_data <- complete(mice(housing_data, printFlag = FALSE))
```

```{r variable transformation}
housing_data <- housing_data %>% 
  mutate(avg_dist = mean(dist1 + dist2 + dist3 + dist4)) %>% 
  select(-c(dist1, dist2, dist3, dist4))

```

```{r creating dummy variables}
# Creating dummy variables using the dummies pacakge
housing_data <- tibble(dummy.data.frame(housing_data)) %>% 
  select(-c(airportNO, waterbodyNone))
```

```{r simple linear regression}
# Fitting a simpole lm
fit_1 <- lm(price ~ room_num, data = housing_data)

summary(fit_1)

housing_data %>% 
  ggplot(aes(x = room_num, y = price)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE)

```

```{r multiple linear regerssion}
# Fitting a multiple lm with all variables
fit_2 <- lm(price ~ ., data = housing_data)

summary(fit_2)

```

```{r train test split}
housing_data <- housing_data %>% 
  mutate(Sold = as_factor(Sold))

# Creating a train test split using rsample
housing_data_split <- initial_split(housing_data, .8)

housing_train <- training(housing_data_split)
housing_test <- testing(housing_data_split)

# Model and evaluation
fit_3 <- lm(price ~ ., data = housing_train)

train_pred <- predict(fit_3, newdata = housing_train)
test_pred <- predict(fit_3, newdata = housing_test)

train_mse <- mean((housing_train$price - train_pred)^2)
test_mse <- mean((housing_test$price - test_pred)^2)

train_mse
test_mse
```

```{r first test}
# Reading in the car data
car_data <- read_xlsx("Cardata.xlsx") %>% 
  clean_names()

# Making a correlation matrix plot
ggcorrplot(cor(car_data), hc.order = TRUE, type = "lower",
           outline.color = "white",
           ggtheme = theme_light,
           colors = c("#6D9EC1", "white", "#E46726"))

# Split
car_data_split <- initial_split(car_data, 0.8)
car_train <- training(car_data_split)
car_test <- testing(car_data_split)

# Model
fit_car <- lm(mpg ~ ., data = car_train)

summary(fit_car)

car_train_pred <- predict(fit_car, newdata = car_train)
car_test_pred <- predict(fit_car, newdata = car_test)

car_train_mse <- mean((car_train$mpg - car_train_pred)^2)
car_test_mse <- mean((car_test$mpg - car_test_pred)^2)

car_train_mse
car_test_mse
```

```{r best subset selection}
library(leaps)

lm_best <- regsubsets(price ~ ., data = housing_data, nvmax = 15)
which.max(summary(lm_best)$adjr2)
coef(lm_best, 8)

lm_forward <- regsubsets(price ~ ., data = housing_data, nvmax = 15, method = "forward")
which.max(summary(lm_forward)$adjr2)
coef(lm_forward, 9)

lm_backward <- regsubsets(price ~ ., data = housing_data, nvmax = 15, method = "backward")
which.max(summary(lm_backward)$adjr2)
coef(lm_backward, 9)
```

```{r shrinkage methods}
# Need to find way to do this with tidymodels

# Ridge
lm_ridge <- linear_reg(mixture = 0) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") %>% 
  fit(price ~ ., data = housing_data)

summary(lm_ridge)

# Lasso
lm_lasso <- linear_reg(mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") %>% 
  fit(price ~ ., data = housing_data)

summary(lm_lasso)
```

```{r logistic regression}
glm_fit <- glm(Sold ~ price, family = binomial, data = housing_train)

summary(glm_fit)

glm_fit_2 <- glm(Sold ~ ., family = binomial, data = housing_train)

summary(glm_fit_2)

glm_pred <- predict(glm_fit, type = "response")
```

```{r Linear Discriminant Analysis}
lda_fit <- MASS::lda(Sold ~ ., data = housing_train %>% select(-avg_dist))

lda_pred <- predict(lda_fit, housing_train)
```

```{r knn classification}
# Setting the seed for reproducibility
set.seed(2001)

# Creating 5 folds
#housing_folds <- vfold_cv(housing_train, v = 5)

# Creating the knn model
knn_model <- nearest_neighbor(neighbors = 3) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

# Creating a recipe for normalization
knn_recipe <- recipe(Sold ~ ., data = housing_train) %>% 
  step_normalize(all_numeric(), -all_outcomes())

# Creating a workflow
knn_workflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(knn_recipe)

# Creating a grid for tuning
#k_grid <- tibble(neighbors = c(10, 20, 30, 50, 75, 100, 125, 150))

# Tuning
#knn_tuning <- knn_workflow %>% 
#  tune_grid(resamples = housing_folds, grid = k_grid)

# Finding the best tune
#knn_tuning %>% 
#  show_best('roc_auc')

# Showing the best tune
#best_k <- knn_tuning %>% 
#  select_best(metric = 'roc_auc')

#best_k

# Selecting the best tune
#final_knn_workflow <- knn_workflow %>% 
#  finalize_workflow(best_k)

# Fitting on test data
last_fit_knn <- knn_workflow %>% 
  last_fit(split = housing_data_split)

# Evaluating the results
last_fit_knn %>% 
  collect_metrics()

# Getting the predictions
knn_predictions <- last_fit_knn %>% 
  collect_predictions()

# ROC plot
knn_predictions %>% 
  roc_curve(truth = Sold, estimate = .pred_yes) %>% 
  autoplot()

# Confusion matrix
conf_mat(knn_predictions, truth = Sold, estimate = .pred_class)
```

